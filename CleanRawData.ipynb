{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7753f993",
   "metadata": {},
   "source": [
    "# This notebook performs the following tasks:\n",
    "\n",
    "- Retrieves job postings data from AWS DynamoDB and S3 and merges them, removing duplicates and identifying unprocessed job postings.\n",
    "- Cleans job descriptions, extracts hard skills, and removes duplicated job postings based on company, title, location, and hard skills.\n",
    "- Categorizes job titles into data scientist, data engineer, data analyst, and software engineer categories, and generates embeddings for hard skills.\n",
    "- Categorizes job locations into USA, Canada, or remote.\n",
    "Transforms the data by converting timezones, formatting dates, and saving the transformed DataFrame to an Amazon S3 bucket as a CSV file.\n",
    "\n",
    "### Key outputs:\n",
    "\n",
    "- Total job postings, processed job postings, and unprocessed job postings.\n",
    "- DataFrame with cleaned job descriptions, extracted hard skills, categorized job titles, and categorized locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df815266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sentence-transformers in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (1.8.1)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (0.14.1)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (4.63.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (4.27.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (1.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from torchvision->sentence-transformers) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.8)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: papermill in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.4.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from papermill) (5.4.1)\n",
      "Requirement already satisfied: tenacity in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from papermill) (8.1.0)\n",
      "Requirement already satisfied: nbclient>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from papermill) (0.7.2)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from papermill) (8.1.3)\n",
      "Requirement already satisfied: tqdm>=4.32.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from papermill) (4.63.2)\n",
      "Requirement already satisfied: nbformat>=5.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from papermill) (5.7.1)\n",
      "Requirement already satisfied: entrypoints in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from papermill) (0.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from papermill) (2.28.1)\n",
      "Requirement already satisfied: ansiwrap in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from papermill) (0.8.4)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from nbclient>=0.2.0->papermill) (5.1.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from nbclient>=0.2.0->papermill) (7.4.8)\n",
      "Requirement already satisfied: traitlets>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from nbclient>=0.2.0->papermill) (5.8.1)\n",
      "Requirement already satisfied: fastjsonschema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from nbformat>=5.1.2->papermill) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from nbformat>=5.1.2->papermill) (3.2.0)\n",
      "Requirement already satisfied: textwrap3>=0.9.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from ansiwrap->papermill) (0.9.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->papermill) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->papermill) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->papermill) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->papermill) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (22.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (0.19.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (65.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (2.8.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (1.5.5)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (6.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: platformdirs>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->nbclient>=0.2.0->papermill) (2.6.2)\r\n"
     ]
    }
   ],
   "source": [
    "# install sentence transformer and paprmill\n",
    "!pip install -U sentence-transformers\n",
    "!pip install papermill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8f4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install libraries\n",
    "import json\n",
    "import boto3\n",
    "import re\n",
    "import hashlib\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "import re\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d7f81",
   "metadata": {},
   "source": [
    "## This code performs the following tasks:\n",
    "\n",
    "- Connects to an AWS DynamoDB database and scans a table named \"TransformedGoogleJob\" to retrieve all items.\n",
    "- Converts the retrieved data to a Pandas DataFrame.\n",
    "- Connects to an AWS S3 bucket named \"embeddedgooglejob\".\n",
    "- Lists all files in the S3 bucket and reads all CSV files.\n",
    "- Concatenates all CSV files into a single DataFrame named \"combined_dataframe\".\n",
    "- Drops duplicate rows from the \"combined_dataframe\" based on the \"id\" column and creates a new DataFrame named \"processed_df\".\n",
    "- Merges the \"total_df\" and \"processed_df\" DataFrames on the \"id\" column using a left join.\n",
    "- Selects all rows in the merged DataFrame where the \"_merge\" column has a value of \"left_only\", indicating that they were not processed before.\n",
    "- Drops the \"_merge\" column from the resulting DataFrame and creates a new DataFrame named \"unprocessed_df\".\n",
    "- Copies the \"unprocessed_df\" DataFrame into a new DataFrame named \"df\".\n",
    "- The final output of the code is the number of total job postings, the number of processed job postings, and the number of unprocessed job postings. The \"df\" DataFrame can be used to further process the unprocessed job postings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0ac3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total job_postings are 3499\n",
      "Number of proceessed job_postings are 3444\n",
      "Number of unprocessed job_postings are 55\n"
     ]
    }
   ],
   "source": [
    "# Create a DynamoDB client\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "table = dynamodb.Table('TransformedGoogleJob')\n",
    "\n",
    "# Scan the table\n",
    "response = table.scan()\n",
    "data = response['Items']\n",
    "\n",
    "# Iterate and retrieve all items (if there are more than the initial limit)\n",
    "while 'LastEvaluatedKey' in response:\n",
    "    response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "    data.extend(response['Items'])\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "total_df = pd.DataFrame(data)\n",
    "\n",
    "print('Number of total job_postings are',len(total_df))\n",
    "\n",
    "bucket_name = \"embeddedgooglejob\"\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "\n",
    "def list_files_in_bucket(bucket):\n",
    "    files = []\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    for page in paginator.paginate(Bucket=bucket):\n",
    "        for obj in page['Contents']:\n",
    "            files.append(obj['Key'])\n",
    "    return files\n",
    "\n",
    "def read_csv_from_s3(bucket, file_key):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=file_key)\n",
    "    csv_content = obj['Body'].read().decode('utf-8')\n",
    "    return pd.read_csv(StringIO(csv_content))\n",
    "\n",
    "all_files = list_files_in_bucket(bucket_name)\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "all_dataframes = []\n",
    "for file_key in all_files:\n",
    "    data = read_csv_from_s3(bucket_name, file_key)\n",
    "    all_dataframes.append(data)\n",
    "\n",
    "# Concatenate all CSV files into a single DataFrame and drop duplicate rows\n",
    "combined_dataframe = pd.concat(all_dataframes, ignore_index=True)\n",
    "# Drop duplicate rows\n",
    "processed_df = combined_dataframe.drop_duplicates(subset=['id'],keep='first',ignore_index=True)\n",
    "print('Number of proceessed job_postings are',len(processed_df))\n",
    "\n",
    "# Merge the processed and total DataFrames and select unprocessed rows\n",
    "unprocessed_df = total_df.merge(processed_df[['id']], on='id', how='left', indicator=True).query(\"_merge == 'left_only'\").drop('_merge', axis=1)\n",
    "print('Number of unprocessed job_postings are',len(unprocessed_df))\n",
    "# Create a copy of the unprocessed DataFrame\n",
    "df=unprocessed_df.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ef2cf",
   "metadata": {},
   "source": [
    "## This code performs the following tasks:\n",
    "\n",
    "- A list of job skills is defined, containing various skills related to data engineering, data science, and data analytics.\n",
    "- A function named \"clean_job_description\" is defined, which takes a job description text as input and performs various cleaning operations, such as removing stop words, removing punctuation (except for plus, hash, and backslash), preserving C++ and C# syntax, and keeping numbers and letters.\n",
    "- Another function named \"extract_hardskills\" is defined, which takes a job description and the list of job skills as input and returns a list of matching hard skills found in the description. This function also calls the \"clean_job_description\" function to clean the job description before extracting the hard skills.\n",
    "- The \"hard_skills\" column is added to the \"df\" DataFrame by applying the \"extract_hardskills\" function to the \"description\" column.\n",
    "- Duplicated rows in the DataFrame are identified based on the \"company\", \"title\", and \"location\" columns.\n",
    "- Duplicated rows are grouped by \"company\", \"title\", and \"location\".\n",
    "- The indices of the duplicated rows are printed, organized by \"company\", \"title\", and \"location\".\n",
    "- A new column named \"hard_skill_tuple\" is added to the \"df\" DataFrame by applying the \"tuple\" function to the \"hard_skills\" column.\n",
    "- Duplicate rows in the DataFrame are dropped based on the \"company\", \"title\", \"location\", and \"hard_skill_tuple\" columns, keeping the first occurrence of each duplicate row.\n",
    "- Duplicated rows are again identified based on the \"company\", \"title\", and \"location\" columns.\n",
    "- Duplicated rows are again grouped by \"company\", \"title\", and \"location\".\n",
    "- The indices of the duplicated rows are again printed, organized by \"company\", \"title\", and \"location\".\n",
    "- Overall, this code is designed to clean job descriptions and extract hard skills from them, and then identify and remove duplicated job postings based on various criteria, such as company, title, location, and hard skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7026f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hard skills for Data Engineer is 96, Data Analyst is 34, Data Scientist is 47, Software Engineer is 95\n",
      "Number of total hard skills are 229\n"
     ]
    }
   ],
   "source": [
    "de_skills=['airflow', 'alteryx', 'athena', 'aurora', 'aws', 'azure', 'boto3', 'cassandra', 'cloud computing', 'cloudwatch', 'cockroachdb', 'data factory', 'data lake', 'data lake analytics', 'data management', 'data mapping', 'data migration', 'data mining', 'data munging', 'data pipeline design', 'data pipelines', 'data warehousing', 'database design', 'databricks', 'django', 'docker', 'dynamodb', 'ec2', 'elasticsearch', 'emr', 'etl', 'etl design', 'etl frameworks', 'gcp', 'glue', 'google bigquery', 'google bigtable', 'google cloud compute engine', 'google cloud dataflow', 'google cloud functions', 'google cloud kubernetes engine', 'google cloud pub/sub', 'google cloud spanner', 'google cloud storage', 'google storage', 'hadoop', 'hbase', 'hive', 'impala', 'informatica', 'kinesis', 'knime', 'kubernetes', 'lambda', 'mongodb', 'mpi', 'mysql', 'olap', 'oltp', 'oracle', 'pandas', 'pentaho', 'pig', 'pl/sql', 'postgresql', 'power bi', 'pyspark', 'rds', 'redis', 'redshift', 'relational databases', 's3', 'sas', 'sas enterprise guide', 'scalable systems', 'schema design', 'snowflake', 'solr', 'spark', 'sql', 'sql anywhere', 'sql server', 'sql*plus', 'sql/psm', 'sqlite', 'ssis','scala', 'stata', 'sybase', 'talend', 'teradata', 'unix', 'yarn','distributed systems', 'mariadb', 'performance tuning']\n",
    "\n",
    "da_skills=['agile', 'bayesian networks', 'classification', 'clustering', 'collaborative filtering', 'cognos', 'dimensional modeling', 'dimensionality reduction', 'excel', 'google analytics', 'hypothesis testing', 'looker', 'mathematics', 'matplotlib', 'power bi', 'predictive modeling', 'probability', 'r', 'random forest', 'regression analysis', 'scrum', 'spectral clustering', 'spotfire', 'sql', 'statistical analysis', 'tableau', 'text mining', 'tidyverse', 'time series analysis', 'unsupervised learning', 'web scraping', 'weka','kibana', 'splunk']\n",
    "\n",
    "ds_skills=['bayesian networks', 'classification', 'clustering', 'collaborative filtering', 'convolutional neural networks', 'cuda', 'deep learning', 'dimensionality reduction', 'ensemble modeling', 'feature engineering', 'h2o', 'hypothesis testing', 'keras', 'machine learning','matlab', 'machine vision', 'mapreduce', 'mathematics', 'matplotlib', 'natural language processing', 'neural networks', 'nlp', 'nltk', 'numpy', 'opencv', 'pandas', 'predictive modeling', 'probability', 'random forest', 'regression analysis', 'recommender systems', 'scikit-learn', 'singular value decomposition','statistical analysis', 'support vector machines', 'tensorflow', 'text mining', 'theano', 'time series analysis', 'torch', 'unsupervised learning', 'Large Language Models', 'natural language processing', 'nlp', 'nltk', 'pytorch','logistic regression']\n",
    "\n",
    "se_skills=['app development', 'agile', 'android app development', 'angularjs', 'ariflow', 'aws', 'azure', 'clojure', 'cloud computing', 'cloudwatch', 'cuda', 'd3js', 'dask', 'devops', 'django', 'docker', 'electron', 'erlang', 'expressjs', 'fastapi', 'fiori/ui5', 'flask', 'git', 'go', 'gradle', 'graphql', 'grunt', 'gulp', 'haskell', 'hibernate', 'html/css', 'java', 'javafx', 'javascript', 'jenkins', 'jquery', 'junit', 'jupyter notebook', 'kotlin', 'kubernetes', 'lambda', 'lamp stack', 'laravel', 'linux', 'microservices', 'mockito', 'nodejs', 'nosql', 'object oriented abap', 'object-oriented programming', 'objective-c', 'opencv', 'opengl', 'openmp', 'perl', 'php', 'posix threads', 'python', 'qt', 'r', 'rabbitmq', 'rdbms', 'reactjs', 'rest apis', 'ruby', 'ruby on rails', 'rust', 'selenium', 'shell scripting', 'spring', 'spring framework', 'sql', 'sql server', 'swift', 'symfony', 't-sql', 'tableau', 'transact-sql', 't-sql', 'typescript', 'unity', 'unix', 'vuejs', 'web scraping', 'web services', 'webpack', 'xml','distributed systems', 'maven', 'performance tuning', 'pygame', 'pyqt', 'scala', 'tkinter', 'stl']\n",
    "\n",
    "job_skills=list(set(de_skills+da_skills+ds_skills+se_skills))\n",
    "print(\"Number of hard skills for Data Engineer is {}, Data Analyst is {}, Data Scientist is {}, Software Engineer is {}\".format(len(de_skills),len(da_skills),len(ds_skills),len(se_skills)))\n",
    "print(\"Number of total hard skills are {}\".format(len(job_skills)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9259db84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: BenchSci, Title: Engineering Manager, Data, Location: Toronto, ON, Canada\n",
      "Indices: [5, 1893]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hard Skills for DE, DS, DA, DE\n",
    "\n",
    "\n",
    "\n",
    "def clean_job_description(text):\n",
    "    \"\"\"\n",
    "    This function takes a job description text and performs the following cleaning operations:\n",
    "    - Remove punctuation\n",
    "    - Remove stop words\n",
    "    - Preserve C++ and C# syntax\n",
    "    - Keep numbers and letters\n",
    "    :param text: A string of job description text\n",
    "    :return: A string of cleaned job description text\n",
    "    \"\"\"\n",
    "    # Define the stop words to remove\n",
    "    stop_words = [\"a\", \"an\", \"the\", \"and\", \"or\", \"of\", \"to\", \"in\", \"for\", \"on\", \"with\", \"at\", \"by\", \"from\", \"up\", \"as\",\n",
    "                  \"it\", \"its\", \"their\", \"they\", \"them\", \"he\", \"him\", \"his\", \"she\", \"her\", \"hers\", \"you\", \"your\", \"yours\",\n",
    "                  \"we\", \"us\", \"our\", \"ours\", \"me\", \"my\", \"mine\", \"i\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\n",
    "                  \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"will\", \"would\", \"shall\",\n",
    "                  \"should\", \"may\", \"might\", \"must\", \"can\", \"could\", \"to\", \"that\", \"this\", \"there\", \"these\", \"those\"]\n",
    "    \n",
    "    # Remove punctuation except for plus, hash, backslah \n",
    "    text = re.sub(r'[^\\w\\s+#\\/]', '', text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "\n",
    "    # Remove stop words\n",
    "    text = ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
    "    text=text.lower()\n",
    "    return text\n",
    "\n",
    "def extract_hardskills(description, job_skills):\n",
    "    description = clean_job_description(description)\n",
    "\n",
    "    # Loop through the list of job postings to extract c# and c++\n",
    "    pattern = r'\\bc\\s*\\+\\+|\\bc#|\\bc\\b'\n",
    "\n",
    "    # Create a new list of job skills that are in the words list\n",
    "    matching_skills = list(set([skill for skill in job_skills if re.search(rf\"\\b{re.escape(skill)}\\b\", description)]+re.findall(pattern, description)))\n",
    "\n",
    "    return matching_skills\n",
    "\n",
    "#Extract Hard skills from description and add it to dataframe\n",
    "df['hard_skills'] = df['description'].apply(extract_hardskills, args=(job_skills,))\n",
    "\n",
    "# Find duplicated rows based on 'company', 'title', and 'location' columns\n",
    "duplicated_rows = df[df.duplicated(subset=['company', 'title', 'location'], keep=False)]\n",
    "\n",
    "# Group duplicated rows by 'company', 'title', and 'location'\n",
    "grouped_duplicated_rows = duplicated_rows.groupby(['company', 'title', 'location'])\n",
    "\n",
    "# Print the indices of the duplicated rows, organized by 'company', 'title', and 'location'\n",
    "for (company, title, location), group in grouped_duplicated_rows:\n",
    "    print(f\"Company: {company}, Title: {title}, Location: {location}\")\n",
    "    print(f\"Indices: {list(group.index)}\")\n",
    "    print()\n",
    "\n",
    "df['hard_skill_tuple'] = df['hard_skills'].apply(tuple)\n",
    "df.drop_duplicates(subset=['company', 'title', 'location', 'hard_skill_tuple'], keep='first', inplace=True)\n",
    "df.drop('hard_skill_tuple', axis=1, inplace=True)\n",
    "\n",
    "# Find duplicated rows based on 'company', 'title', and 'location' columns\n",
    "duplicated_rows = df[df.duplicated(subset=['company', 'title', 'location'], keep=False)]\n",
    "\n",
    "# Group duplicated rows by 'company', 'title', and 'location'\n",
    "grouped_duplicated_rows = duplicated_rows.groupby(['company', 'title', 'location'])\n",
    "\n",
    "# Print the indices of the duplicated rows, organized by 'company', 'title', and 'location'\n",
    "for (company, title, location), group in grouped_duplicated_rows:\n",
    "    print(f\"Company: {company}, Title: {title}, Location: {location}\")\n",
    "    print(f\"Indices: {list(group.index)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a4fc5",
   "metadata": {},
   "source": [
    "## This code defines lists of job titles for different job categories, such as data scientist, data engineer, data analyst, and software engineer.\n",
    "\n",
    "It then defines a function categorize_job_title that takes a job title as input and returns the job category it belongs to based on the lists of job titles defined earlier.\n",
    "\n",
    "The code then applies the categorize_job_title function to the 'title' column of a Pandas DataFrame called df to create a new column 'job_category', which categorizes each job title in the DataFrame into one of the job categories.\n",
    "\n",
    "The code then uses the SentenceTransformer library to convert the list of hard skills for each job into a single string and then into an embedding. The embedding is added as a new column 'embedding' in the DataFrame.\n",
    "\n",
    "Finally, the code defines a function categorize_location that takes a location string and returns whether the job is in the USA, Canada, or is remote. The function is applied to the 'location' column of the DataFrame to create a new column 'location_category'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1945cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncategorized title: Engineering Manager, Data\n",
      "Uncategorized title: Data Center Electrical Engineer, Google Data Centers\n",
      "Uncategorized title: ERP Programmer (MS Dynamics)\n",
      "Uncategorized title: Axiom Developer\n",
      "Uncategorized title: Insight Global\n",
      "Uncategorized title: Unreal Programmer\n",
      "Uncategorized title: Insight Global\n",
      "Uncategorized title: Engineering Manager\n",
      "Uncategorized title: Army Information Network Engineer\n",
      "Uncategorized title: Assc Dir-Solutions Specialist\n",
      "Uncategorized title: ER Data Center Process Principal (RNTBD)\n",
      "Uncategorized title: Principal Data Consultant\n",
      "Uncategorized title: Data Champion\n",
      "Uncategorized title: Product Manager, Data Strategy\n",
      "Uncategorized title: Platform Support Engineer\n",
      "Uncategorized title: Product Owner - MLOps\n",
      "Uncategorized title: Staff Engineer - Canada\n",
      "Uncategorized title: Senior Service Engineer\n",
      "Uncategorized title: Data Solution Partner (Part Time)\n",
      "Uncategorized title: Angular Developer\n",
      "Uncategorized title: Deep Learning Compiler Engineer II, AWS Neuron, Annapurna Labs\n",
      "Uncategorized title: Engineering System Engineer\n",
      "Uncategorized title: Engineering Manager, Compute\n",
      "Uncategorized title: Tableau Developer\n",
      "Uncategorized title: Senior Network Engineer\n",
      "Uncategorized title: BHJOB15656_18493 - Packet Core Engineer\n",
      "Uncategorized title: Senior Data Consultant\n",
      "Uncategorized title: Senior Project Engineer - -\n",
      "Uncategorized title: ServiceNow Developer\n",
      "Uncategorized title: Senior Data Protection Engineer- Hybrid\n",
      "Uncategorized title: QA Automation Engineer\n",
      "Uncategorized title: Senior R&D Engineer - Thermohydraulics and Thermal Systems *Remote\n",
      "Uncategorized title: Senior Mobile Build and Release Engineer\n",
      "Uncategorized title: Bilingual ServiceNow Developers\n",
      "Uncategorized title: R&D Engineer II - Thermal Systems *Remote\n",
      "Uncategorized title: Data Network Engineer\n",
      "Uncategorized title: Sr. BIM/Revit Developer (Canada only) - Freelance [Remote]\n",
      "Uncategorized title: Accountant\n",
      "Uncategorized title: Principal Engineer, Network Virtualization, Google Cloud Platform\n",
      "Uncategorized title: Data Quality Manager\n",
      "Uncategorized title: Data Visualization Engineer\n"
     ]
    }
   ],
   "source": [
    "data_scientist_titles = [\"Data Science\",\n",
    "    \"Data Scientist\",\"DataScientist\",\n",
    "    \"Machine Learning Engineer\",\n",
    "    \"AI Engineer\",\n",
    "    \"AI Researcher\",\n",
    "    \"Machine Learning Researcher\",\n",
    "    \"Computer Vision Engineer\",\n",
    "    \"Natural Language Processing Engineer\",\n",
    "    \"Deep Learning Engineer\",\n",
    "    \"Data Science Specialist\",\n",
    "    \"Statistical Modeler\",\n",
    "    \"Applied Scientist\",\n",
    "    \"Research Scientist\",\n",
    "    \"Quantitative Analyst\",\n",
    "    \"AI Scientist\",\n",
    "    \"Algorithm Developer\",\n",
    "    \"Machine Learning Scientist\",\n",
    "    \"AI Developer\",\n",
    "    \"Data Science Engineer\",\n",
    "    \"Machine Learning Specialist\",\n",
    "    \"Data Science Analyst\",\n",
    "    \"Artificial Intelligence Engineer\",\n",
    "    \"Deep Learning Specialist\",\n",
    "    \"Predictive Modeler\",\n",
    "    \"Quantitative Researcher\",\n",
    "    \"AI Consultant\",\n",
    "    \"Reinforcement Learning Engineer\",\n",
    "    \"Data Science Consultant\",\n",
    "    \"Machine Learning Analyst\",\n",
    "    \"AI Analyst\",\n",
    "    \"Data Science Researcher\",\n",
    "    \"ML Engineer\",\"Machine Learning\",\"ML Ops Engineer\",\n",
    "    \"ML Researcher\",\n",
    "    \"Data Mining Engineer\",\n",
    "    \"Data Science Developer\",\n",
    "    \"AI Specialist\",\n",
    "    \"Algorithm Engineer\",\n",
    "    \"Deep Learning Researcher\",\n",
    "    \"Machine Intelligence Engineer\",\n",
    "    \"Data Analysis Scientist\",\n",
    "    \"Advanced Analytics Engineer\"\n",
    "]\n",
    "data_engineer_titles = [\"Cloud Engineer\",\"Engineer, Data\",\"ETL Developer\",\"Data Ops Lead\",\"Data Schema Specialist\",\n",
    "    \"Data Engineer\",\"Data Developer\",\n",
    "    \"Big Data Engineer\",\n",
    "    \"Data Integration Engineer\",\n",
    "    \"Data Pipeline Engineer\",\n",
    "    \"Data Platform Engineer\",\n",
    "    \"ETL Engineer\",\n",
    "    \"Big Data Developer\",\n",
    "    \"Data Infrastructure Engineer\",\n",
    "    \"Data Warehouse Engineer\",\n",
    "    \"Streaming Data Engineer\",\n",
    "    \"Data Engineering Manager\",\n",
    "    \"Database Engineer\",\n",
    "    \"Cloud Data Engineer\",\n",
    "    \"Azure Data Engineer\",\n",
    "    \"AWS Data Engineer\",\n",
    "    \"GCP Data Engineer\",\n",
    "    \"Hadoop Engineer\",\n",
    "    \"Data Architect\",\n",
    "    \"Big Data Architect\",\n",
    "    \"Data Engineering Consultant\",\n",
    "    \"Data Operations Engineer\",\n",
    "    \"Data Systems Engineer\",\n",
    "    \"Data Analytics Engineer\",\n",
    "    \"Real-time Data Engineer\",\n",
    "    \"Spark Engineer\",\n",
    "    \"NoSQL Engineer\",\n",
    "    \"Data Lake Engineer\",\n",
    "    \"Data Management Engineer\",\n",
    "    \"Data Storage Engineer\",\n",
    "    \"Distributed Data Engineer\",\n",
    "    \"Data Processing Engineer\",\n",
    "    \"Big Data Solutions Engineer\",\n",
    "    \"Data Strategy Engineer\",\n",
    "    \"Data Science Platform Engineer\",\n",
    "    \"Data Engineering Analyst\",\n",
    "    \"Data Modeler\",\n",
    "    \"Data Integration Architect\",\n",
    "    \"Data Engineering Specialist\",\n",
    "    \"Data Migration Engineer\",\n",
    "    \"DevOps Engineer\",\n",
    "    \"Data Engineering Lead\"\n",
    "]\n",
    "\n",
    "data_analyst_titles = [\"Data Analytics\",\"Data Management Analyst\",\"Analytic\",\"Analyst\",\n",
    "    \"Data Analysis\",\n",
    "    \"Data Analyst\",\n",
    "    \"Business Data Analyst\",\n",
    "    \"Data Analytics Specialist\",\n",
    "    \"Marketing Data Analyst\",\n",
    "    \"Financial Data Analyst\",\n",
    "    \"Operations Data Analyst\",\n",
    "    \"Quantitative Data Analyst\",\n",
    "    \"Healthcare Data Analyst\",\n",
    "    \"Data Reporting Analyst\",\n",
    "    \"Sales Data Analyst\",\n",
    "    \"Customer Data Analyst\",\n",
    "    \"Data Quality Analyst\",\n",
    "    \"Data Visualization Analyst\",\n",
    "    \"HR Data Analyst\",\n",
    "    \"Supply Chain Data Analyst\",\n",
    "    \"Risk Data Analyst\",\n",
    "    \"Data Insights Analyst\",\n",
    "    \"Data Analyst Intern\",\n",
    "    \"Data Governance Analyst\",\n",
    "    \"Data Analyst Consultant\",\n",
    "    \"SQL Data Analyst\",\n",
    "    \"Data Analyst Manager\",\n",
    "    \"Data Research Analyst\",\n",
    "    \"Data Analyst Lead\",\n",
    "    \"Junior Data Analyst\",\n",
    "    \"Senior Data Analyst\",\n",
    "    \"Data Analyst Associate\",\n",
    "    \"Data Analyst Coordinator\",\n",
    "    \"Database Analyst\",\n",
    "    \"Data Audit Analyst\",\n",
    "    \"Data Intelligence Analyst\",\n",
    "    \"Fraud Data Analyst\",\n",
    "    \"Data Integration Analyst\",\n",
    "    \"Data Systems Analyst\",\n",
    "    \"Data Analyst I\",\n",
    "    \"Data Analyst II\",\n",
    "    \"Entry-Level Data Analyst\",\n",
    "    \"Strategic Data Analyst\"\n",
    "]\n",
    "\n",
    "software_engineer_titles = [\"Software\",\"Frontend\",\"Backend\",\"Python Engineer\",\"Back End Developer\",\".Net Developer\",\n",
    "    \"Software Engineer\",\"Senior Fullstack Developer\",\".NET/Angular Engineer\",\n",
    "    \"Software Developer\",\"MAC Engineer with Xcode/Swift\",\"Android Engineer\",\"Back End Engineer\",\n",
    "    \"Web Developer\",\n",
    "    \"Full Stack Developer\",\n",
    "    \"Backend Developer\",\n",
    "    \"Frontend Developer\",\n",
    "    \"Mobile Developer\",\n",
    "    \"iOS Developer\",\n",
    "    \"Android Developer\",\n",
    "    \"Embedded Software Engineer\",\n",
    "    \"Software Engineer Intern\",\n",
    "    \"Junior Software Engineer\",\n",
    "    \"Senior Software Engineer\",\n",
    "    \"Software Architect\",\n",
    "    \"Application Developer\",\n",
    "    \"Java Developer\",\"Java Engineer\",\n",
    "    \"Python Developer\",\n",
    "    \"C++ Developer\",\n",
    "    \"C# Developer\",\n",
    "    \"JavaScript Developer\",\n",
    "    \"Ruby Developer\",\n",
    "    \"PHP Developer\",\n",
    "    \"Golang Developer\",\n",
    "    \"Scala Developer\",\n",
    "    \"Swift Developer\",\n",
    "    \"Kotlin Developer\",\n",
    "    \"Rust Developer\",\n",
    "    \"Software Development Engineer\",\n",
    "    \"Software Engineering Manager\",\n",
    "    \"Software Test Engineer\",\n",
    "    \"Game Developer\",\n",
    "    \"UI Developer\",\n",
    "    \"UX Developer\",\n",
    "    \"Frontend Engineer\",\n",
    "    \"Backend Engineer\",\n",
    "    \"Full Stack Engineer\",\n",
    "    \"Cloud Developer\",\n",
    "    \"Systems Software Engineer\",\n",
    "    \"Software Development Analyst\"\n",
    "]\n",
    "\n",
    "\n",
    "job_categories = {\n",
    "        'data engineer': data_engineer_titles,\n",
    "        'data scientist': data_scientist_titles,\n",
    "        'data analyst': data_analyst_titles,\n",
    "        'software engineer': software_engineer_titles\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to categorize the job titles\n",
    "def categorize_job_title(title):\n",
    "    for category, titles in job_categories.items():\n",
    "        if any(t.lower() in title.lower() for t in titles):\n",
    "            return category\n",
    "    print(f\"Uncategorized title: {title}\")\n",
    "    return None\n",
    "\n",
    "# Add a new column 'job_category' to the DataFrame\n",
    "df['job_category'] = df['title'].apply(categorize_job_title)\n",
    "\n",
    "# Filter the DataFrame to include only the desired job categories\n",
    "df = df[df['job_category'].isin(job_categories.keys())]\n",
    "\n",
    "#initialize Bert Model\n",
    "# Most powerful\n",
    "model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "# Lightweight\n",
    "#model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "# Convert skills list to a single string to use sentence_transformer\n",
    "def skills_to_string(skills_list, separator=', '):\n",
    "    return separator.join(skills_list)\n",
    "\n",
    "\n",
    "for index, job in df.iterrows():\n",
    "    df.loc[index, 'STRskills'] = skills_to_string(job['hard_skills'])\n",
    "    \n",
    "\n",
    "def encode_skills(skills):\n",
    "    return model.encode(skills, convert_to_numpy=True).tolist()\n",
    "\n",
    "df['embedding'] = df['STRskills'].apply(lambda x: encode_skills(x))\n",
    "\n",
    "#categorize into USA vs Canada vs Remote\n",
    "def categorize_location(location):\n",
    "    if 'Canada' in location or ', Canada' in location or 'Canad√°' in location or 'CA' in location:\n",
    "        return 'Canada'\n",
    "    elif 'Anywhere' in location or 'Qualquer lugar' in location:\n",
    "        return 'Remote'\n",
    "    else:\n",
    "        return 'USA'\n",
    "\n",
    "df['location_category'] = df['location'].apply(categorize_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e716f",
   "metadata": {},
   "source": [
    "## Data Transformation Script Overview\n",
    "This code is a data transformation script that applies several functions to a DataFrame object, converts the timezones, formats the dates, and then saves the transformed DataFrame into an Amazon S3 bucket as a CSV file.\n",
    "\n",
    "## Function Description\n",
    "The convert_posted_at_to_local_time() function takes a timestamp in the format \"2 days ago\" or \"1 hour ago\" and converts it to the local time. It uses the datetime module to get the current time in UTC and convert it to the local time. It then parses the input timestamp, calculates the difference between the current time and the input timestamp and returns the local posted date in the format \"YYYY-MM-DD HH:MM:SS\".\n",
    "\n",
    "## Timezone Conversion\n",
    "The script then defines the Montreal timezone and converts the current UTC time to Montreal local time using the pytz module. It formats the local time as a string and saves it in the variable montreal_time_str.\n",
    "\n",
    "## DataFrame Transformation\n",
    "The next block applies the convert_posted_at_to_local_time() function to the 'metadata' column of the DataFrame and creates a new column 'posted_date' that contains the converted local posted date. It also creates a new column 'scraped_date' that contains the current Montreal time as a string.\n",
    "\n",
    "The script then prints the minimum and maximum posted date to verify that the conversion is working properly.\n",
    "\n",
    "## CSV File Saving\n",
    "The next block constructs a filename for the CSV file to be saved in the S3 bucket. It uses the current Montreal time to generate a filename in the format \"processed_YYYY-MM-DD HH:MM.csv\".\n",
    "\n",
    "The last block saves the transformed DataFrame to the S3 bucket using the boto3 module. It converts the DataFrame to a CSV format, creates a CSV buffer and writes it to the S3 bucket using the put_object() function.\n",
    "\n",
    "Finally, the script prints the total number of processed and saved job postings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94972775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-04-01 22:14:09\n",
      "Total number of new processed and saved job_postings after transformation are, 13\n"
     ]
    }
   ],
   "source": [
    "def convert_posted_at_to_local_time(posted_time):\n",
    "    if not posted_time:\n",
    "        return ''\n",
    "\n",
    "    now = datetime.datetime.now(datetime.timezone.utc)\n",
    "    local_tz = datetime.timezone(datetime.timedelta(hours=-4), 'EDT')\n",
    "    local_now = now.astimezone(local_tz)\n",
    "\n",
    "    match = re.match(r'(\\d+)\\s*(\\w+)\\s*ago', posted_time)\n",
    "    if match:\n",
    "        value, unit = int(match.group(1)), match.group(2)\n",
    "        if unit.startswith('hour'):\n",
    "            local_posted_date = local_now - datetime.timedelta(hours=value)\n",
    "        elif unit.startswith('day'):\n",
    "            local_posted_date = local_now - datetime.timedelta(days=value)\n",
    "        elif unit.startswith('minute'):\n",
    "            local_posted_date = local_now - datetime.timedelta(minutes=value)\n",
    "        else:\n",
    "            local_posted_date = local_now\n",
    "    else:\n",
    "        local_posted_date = local_now\n",
    "\n",
    "    return local_posted_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Get the current time in UTC\n",
    "utc_now = datetime.datetime.now(pytz.utc)\n",
    "\n",
    "# Define the Montreal timezone\n",
    "montreal_tz = pytz.timezone('America/Montreal')\n",
    "\n",
    "# Convert the current time to Montreal local time\n",
    "montreal_now = utc_now.astimezone(montreal_tz)\n",
    "\n",
    "# Format the datetime object as a string\n",
    "montreal_time_str = montreal_now.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "\n",
    "# Apply the function and create the new columns\n",
    "df['posted_date'] = df['metadata'].apply(lambda x: convert_posted_at_to_local_time(x.get('postedAt', '')))\n",
    "df['scraped_date'] = montreal_time_str\n",
    "\n",
    "print(min(df['posted_date']))\n",
    "print(max(df['posted_date']))\n",
    "\n",
    "\n",
    "# Construct the filename\n",
    "file_name = f\"processed_{montreal_time_str}.csv\"\n",
    "\n",
    "bucket_name = \"embeddedgooglejob\"\n",
    "\n",
    "\n",
    "# Save the DataFrame as a CSV file in the S3 bucket\n",
    "s3 = boto3.client('s3')\n",
    "csv_buffer = StringIO()\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "s3.put_object(Bucket=bucket_name, Key=file_name, Body=csv_buffer.getvalue())\n",
    "\n",
    "print('Total number of new processed and saved job_postings after transformation are,', len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
